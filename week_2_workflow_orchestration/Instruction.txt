Nesse capítulo iremos trabalhar com workflow orchestration

O que é Data Lake?

É um repositório com diversos dados, podendo ser estruturado, semi ou não estruturado.
A ideia é armazenar o máximo possível de dado o mais rápido possível.

Data Lake vs Data Warehouse

São dados não estruturados para analistas de dados ou data scientiests.
São dados alimentados todos os dias.

Já o Data Warehouse são dados estruturados, sendo BI e tamanho dos dados é pequeno
e o uso consiste em batch processamento e BI reports.


A ideia é acessar os dados no data lake rapidamente, sendo useful para outras partes do
time.

ETL vs ELT

ETL = Export, transform and load (small amount of data)
ELT = Export, load and transform (used for large amounts of data)

ETL é uma solução para data Warehouse
ELT para é um data lake solution

Data lake converte em um data swamp, isso ocorre pq não há versionamento,
há schemas incompatíveis para o mesmo data sem versionamento
Não é possível fazer joins no data lake
Não há metadata associáveis


---- AULA 2.2.1 workflow orchestration ----

No fim dessa semana será possível fazer um ETL com orchestration open sources

importar flow e task
flow é o objeto mais basico do python

@flow seria para chamar uma função principal e @tasks partes dela

um exemplo:



@task(log_prints=true) #log_prints=True vai mostrar todo o log_prints
  def get_name():
    name = str(input("Type your name: "))
    return name

@flow(name="flow1")
def print_name(name):
  name = get_name()
  print(f'My name is {name}')

if __name__ == "__main__":
    print_name()

Nesse caso o flow chama vários tasks.
Podendo até ter subflows.


Para ver os flows deve-se digitar: "prefect orion start"
Entrar no site e checar todos os flows rodados.
É possível fazer blocks, que seria como armazenar configurações e usá-las em qualquer
outro código. Primeiro deve-se fazer o block no "prefect orion start" e chamá-lo
de volta como se fosse um pip install.


----- Aula 2.2.3 ETL With Prefect and GCP ------

A segunda aula resume-se a fazer outro ingest_data mas enviar para o GCS.

Para isso vamos baixar um conteúdo na internet: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/{color}/{dataset_file}.csv.gz"

Em primeiro momento criaremos um @flow para definir as variáveis "color" e "dataset_file"

O primeiro task será para ler o arquivo .csv
O segundo task será para limpar o arquivo (ETL)
O terceiro task será para converter o arquivo em .parquet
O quarto e último task será para enviar para o Google Cloud Storage (GCS).
Para isso precisamos criar um block com as credenciais do GCS criado (final da semana 1) para acessar o GCS Bucket.
Ao criar o block usamos a chave json criada anteriormente para criar o GCP Credentials.

O código para enviar o arquivo localmente para o GCS é:

def write_gcs(path):
    gcs_block = GcsBucket.load("zoom-gcs")
    gcs_block.upload_from_path(from_path=f"{path}",to_path=path)
    return

Mais informações: https://prefecthq.github.io/prefect-gcp/cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_path

----- Aula 2.2.4 From Google Cloud Storage to Big Query -----

----- Aula 2.2.5 Parametrizing Flow & Deployments with ETL into GCS flow -----

Em primeiro momento apenas criamos um loop para rodar diversos files e mandar arquivos
para o GCS.

Após isso criamos um deployment:

prefect deployment build ./parameterized_flow.py:etl_parameters -n "Parameters ETL"

parameterized_flow.py = nome do arquivo
etl_parameters = nome do flow principal
"Parameters ETL" = nome do deployment

Após mudar a chave {parameters} no arquivo .yaml, mandar o deployment para a API
prefect deployment apply etl_parameters-deployment.yaml

Arrumar os parametros no arquivo yaml e dar run na api ( prefect orion start)

Depois disso rodar agente

prefect agent start  --work-queue "default"
ou
prefect agent start --pool default-agent-pool --work-queue default


----- Aula 2.2.6 Schedules & Docker Storage with Infrastructure -----
